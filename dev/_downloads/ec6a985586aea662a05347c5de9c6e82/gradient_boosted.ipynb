{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gradient-boosted equivalent sources\n\nWhen trying to grid a very large dataset, the regular\n:class:`harmonica.EquivalentSources` might not be the best option: they will\nrequire a lot of memory for storing the Jacobian matrices involved in the\nfitting process of the source coefficients. Instead, we can make use of the\ngradient-boosted equivalent sources, introduced in [Soler2021]_ and available\nin Harmonica through the :class:`harmonica.EquivalentSourcesGB` class. The\ngradient-boosted equivalent sources divide the survey region in overlapping\nwindows of equal size and fit the source coefficients iteratively, considering\nthe sources and data points that fall under each window at a time. The order in\nwhich the windows are visited is randomized to improve convergence of the\nalgorithm.\n\nHere we will produce a grid out of a portion of the ground gravity survey from\nSouth Africa (see :func:`harmonica.datasets.fetch_south_africa_gravity`) using\nthe gradient-boosted equivalent sources. This particlar dataset is not very\nlarge, in fact we could use the :class:`harmonica.EquivalentSources` instead.\nBut we will use the :class:`harmonica.EquivalentSourcesGB` for illustrating how\nto use them on a small example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import boule as bl\nimport matplotlib.pyplot as plt\nimport pyproj\nimport verde as vd\n\nimport harmonica as hm\n\n# Fetch the sample gravity data from South Africa\ndata = hm.datasets.fetch_south_africa_gravity()\n\n# Slice a smaller portion of the survey data to speed-up calculations for this\n# example\nregion = [18, 27, -34.5, -27]\ninside = vd.inside((data.longitude, data.latitude), region)\ndata = data[inside]\nprint(\"Number of data points:\", data.shape[0])\nprint(\"Mean height of observations:\", data.elevation.mean())\n\n# Since this is a small area, we'll project our data and use Cartesian\n# coordinates\nprojection = pyproj.Proj(proj=\"merc\", lat_ts=data.latitude.mean())\neasting, northing = projection(data.longitude.values, data.latitude.values)\ncoordinates = (easting, northing, data.elevation)\n\n# Compute the gravity disturbance\nellipsoid = bl.WGS84\ndata[\"gravity_disturbance\"] = data.gravity - ellipsoid.normal_gravity(\n    data.latitude, data.elevation\n)\n\n# Create the equivalent sources\n# We'll use the block-averaged sources with a block size of 2km and windows of\n# 100km x 100km, a damping of 10 and set the sources at a relative depth of\n# 9km. By specifying the random_state, we ensure to get the same solution on\n# every run.\nwindow_size = 100e3\nblock_size = 2e3\neqs_gb = hm.EquivalentSourcesGB(\n    depth=9e3,\n    damping=10,\n    window_size=window_size,\n    block_size=block_size,\n    random_state=42,\n)\n\n# Let's estimate the memory required to store the largest Jacobian when using\n# these values for the window_size and the block_size.\njacobian_req_memory = eqs_gb.estimate_required_memory(coordinates)\nprint(f\"Required memory for storing the largest Jacobian: {jacobian_req_memory} bytes\")\n\n# Fit the sources coefficients to the observed gravity disturbance.\neqs_gb.fit(coordinates, data.gravity_disturbance)\n\nprint(\"Number of sources:\", eqs_gb.points_[0].size)\n\n# Evaluate the data fit by calculating an R\u00b2 score against the observed data.\n# This is a measure of how well the sources fit the data, NOT how good the\n# interpolation will be.\nprint(\"R\u00b2 score:\", eqs_gb.score(coordinates, data.gravity_disturbance))\n\n# Interpolate data on a regular grid with 2 km spacing. The interpolation\n# requires the height of the grid points (upward coordinate). By passing in\n# 1000 m, we're effectively upward-continuing the data.\ngrid = eqs_gb.grid(\n    upward=1000,\n    spacing=2e3,\n    data_names=\"gravity_disturbance\",\n)\nprint(grid)\n\n# Plot the original gravity disturbance and the gridded and upward-continued\n# version\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 9), sharey=True)\n\n# Get the maximum absolute value between the original and gridded data so we\n# can use the same color scale for both plots and have 0 centered at the white\n# color.\nmaxabs = vd.maxabs(data.gravity_disturbance, grid.gravity_disturbance)\n\nax1.set_title(\"Observed gravity disturbance data\")\ntmp = ax1.scatter(\n    easting,\n    northing,\n    c=data.gravity_disturbance,\n    s=5,\n    vmin=-maxabs,\n    vmax=maxabs,\n    cmap=\"seismic\",\n)\nplt.colorbar(tmp, ax=ax1, label=\"mGal\", pad=0.07, aspect=40, orientation=\"horizontal\")\n\nax2.set_title(\"Gridded with gradient-boosted equivalent sources\")\ntmp = grid.gravity_disturbance.plot.pcolormesh(\n    ax=ax2,\n    add_colorbar=False,\n    add_labels=False,\n    vmin=-maxabs,\n    vmax=maxabs,\n    cmap=\"seismic\",\n)\nplt.colorbar(tmp, ax=ax2, label=\"mGal\", pad=0.07, aspect=40, orientation=\"horizontal\")\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}