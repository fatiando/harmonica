{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gradient-boosted equivalent sources\n\nWhen trying to grid a very large dataset, the regular\n:class:`harmonica.EquivalentSources` might not be the best option: they will\nrequire a lot of memory for storing the Jacobian matrices involved in the\nfitting process of the source coefficients. Instead, we can make use of the\ngradient-boosted equivalent sources, introduced in [Soler2021]_ and available\nin Harmonica through the :class:`harmonica.EquivalentSourcesGB` class. The\ngradient-boosted equivalent sources divide the survey region in overlapping\nwindows of equal size and fit the source coefficients iteratively, considering\nthe sources and data points that fall under each window at a time. The order in\nwhich the windows are visited is randomized to improve convergence of the\nalgorithm.\n\nHere we will produce a grid out of a portion of the ground gravity survey from\nSouth Africa (see :func:`harmonica.datasets.fetch_south_africa_gravity`) using\nthe gradient-boosted equivalent sources. This particlar dataset is not very\nlarge, in fact we could use the :class:`harmonica.EquivalentSources` instead.\nBut we will use the :class:`harmonica.EquivalentSourcesGB` for illustrating how\nto use them on a small example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import boule as bl\nimport ensaio\nimport pandas as pd\nimport pygmt\nimport pyproj\nimport verde as vd\n\nimport harmonica as hm\n\n# Fetch the sample gravity data from South Africa\nfname = ensaio.fetch_southern_africa_gravity(version=1)\ndata = pd.read_csv(fname)\n\n# Slice a smaller portion of the survey data to speed-up calculations for this\n# example\nregion = [18, 27, -34.5, -27]\ninside = vd.inside((data.longitude, data.latitude), region)\ndata = data[inside]\nprint(\"Number of data points:\", data.shape[0])\nprint(\"Mean height of observations:\", data.height_sea_level_m.mean())\n\n# Since this is a small area, we'll project our data and use Cartesian\n# coordinates\nprojection = pyproj.Proj(proj=\"merc\", lat_ts=data.latitude.mean())\neasting, northing = projection(data.longitude.values, data.latitude.values)\ncoordinates = (easting, northing, data.height_sea_level_m)\nxy_region = vd.get_region((easting, northing))\n\n# Compute the gravity disturbance\nellipsoid = bl.WGS84\ndata[\"gravity_disturbance\"] = data.gravity_mgal - ellipsoid.normal_gravity(\n    data.latitude, data.height_sea_level_m\n)\n\n# Create the equivalent sources\n# We'll use the block-averaged sources with a block size of 2km and windows of\n# 100km x 100km, a damping of 10 and set the sources at a relative depth of\n# 9km. By specifying the random_state, we ensure to get the same solution on\n# every run.\nwindow_size = 100e3\nblock_size = 2e3\neqs_gb = hm.EquivalentSourcesGB(\n    depth=9e3,\n    damping=10,\n    window_size=window_size,\n    block_size=block_size,\n    random_state=42,\n)\n\n# Let's estimate the memory required to store the largest Jacobian when using\n# these values for the window_size and the block_size.\njacobian_req_memory = eqs_gb.estimate_required_memory(coordinates)\nprint(f\"Required memory for storing the largest Jacobian: {jacobian_req_memory} bytes\")\n\n# Fit the sources coefficients to the observed gravity disturbance.\neqs_gb.fit(coordinates, data.gravity_disturbance)\n\nprint(\"Number of sources:\", eqs_gb.points_[0].size)\n\n# Evaluate the data fit by calculating an R\u00b2 score against the observed data.\n# This is a measure of how well the sources fit the data, NOT how good the\n# interpolation will be.\nprint(\"R\u00b2 score:\", eqs_gb.score(coordinates, data.gravity_disturbance))\n\n# Interpolate data on a regular grid with 2 km spacing. The interpolation\n# requires the height of the grid points (upward coordinate). By passing in\n# 1000 m, we're effectively upward-continuing the data.\n\ngrid_coords = vd.grid_coordinates(region=xy_region, spacing=2e3, extra_coords=1000)\n\ngrid = eqs_gb.grid(coordinates=grid_coords, data_names=\"gravity_disturbance\")\nprint(grid)\n\n# Set figure properties\nw, e, s, n = xy_region\nfig_height = 10\nfig_width = fig_height * (e - w) / (n - s)\nfig_ratio = (n - s) / (fig_height / 100)\nfig_proj = f\"x1:{fig_ratio}\"\n\n# Plot the original gravity disturbance and the gridded and upward-continued\n# version\nfig = pygmt.Figure()\n\ntitle = \"Observed gravity disturbance data\"\n\n# Make colormap of data\npygmt.makecpt(\n    cmap=\"vik\",\n    series=(\n        -data.gravity_disturbance.quantile(0.99),\n        data.gravity_disturbance.quantile(0.99),\n    ),\n    background=True,\n)\n\nwith pygmt.config(FONT_TITLE=\"14p\"):\n    fig.plot(\n        projection=fig_proj,\n        region=xy_region,\n        frame=[f\"WSne+t{title}\", \"xa200000+a15\", \"ya100000\"],\n        x=easting,\n        y=northing,\n        fill=data.gravity_disturbance,\n        style=\"c0.1c\",\n        cmap=True,\n    )\n\nfig.colorbar(cmap=True, frame=[\"a50f25\", \"x+lmGal\"])\n\nfig.shift_origin(xshift=fig_width + 1)\n\ntitle = \"Gridded with gradient-boosted equivalent sources\"\n\nwith pygmt.config(FONT_TITLE=\"14p\"):\n    fig.grdimage(\n        frame=[f\"ESnw+t{title}\", \"xa200000+a15\", \"ya100000\"],\n        grid=grid.gravity_disturbance,\n        cmap=True,\n    )\n\nfig.colorbar(cmap=True, frame=[\"a50f25\", \"x+lmGal\"])\n\nfig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}