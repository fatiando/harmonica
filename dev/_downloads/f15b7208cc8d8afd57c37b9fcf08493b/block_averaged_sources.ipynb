{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gridding with block-averaged equivalent sources\n\nBy default, the :class:`harmonica.EquivalentSources` class locates one point\nsource beneath each data point during the fitting process. Alternatively, we\ncan use another strategy: the *block-averaged sources*, introduced in\n[Soler2021]_.\n\nThis method divides the survey region (defined by the data) into square blocks\nof equal size, computes the median coordinates of the data points that fall\ninside each block and locates one source beneath every averaged position. This\nway, we define one equivalent source per block, with the exception of empty\nblocks that won't get any source.\n\nThis method has two main benefits:\n\n- It lowers the amount of sources involved in the interpolation, therefore it\n  reduces the computer memory requirements and the computation time of the\n  fitting and prediction processes.\n- It might avoid to produce aliasing on the output grids, specially for\n  surveys with oversampling along a particular direction, like airborne ones.\n\nWe can make use of the block-averaged sources within the\n:class:`harmonica.EquivalentSources` class by passing a value to the\n``block_size`` parameter, which controls the size of the blocks. We recommend\nusing a ``block_size`` not larger than the desired resolution of the\ninterpolation grid.\n\nThe depth of the sources can be set analogously to the regular equivalent\nsources: we can use a ``constant`` depth (every source is located at the same\ndepth) or a ``relative`` depth (where each source is located at a constant\nshift beneath the median location obtained during the block-averaging process).\nThe depth of the sources and which strategy to use can be set up through the\n``depth`` and the ``depth_type`` parameters, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport pyproj\nimport verde as vd\n\nimport harmonica as hm\n\n# Fetch the sample total-field magnetic anomaly data from Great Britain\ndata = hm.datasets.fetch_britain_magnetic()\n\n# Slice a smaller portion of the survey data to speed-up calculations for this\n# example\nregion = [-5.5, -4.7, 57.8, 58.5]\ninside = vd.inside((data.longitude, data.latitude), region)\ndata = data[inside]\nprint(\"Number of data points:\", data.shape[0])\nprint(\"Mean height of observations:\", data.altitude_m.mean())\n\n# Since this is a small area, we'll project our data and use Cartesian\n# coordinates\nprojection = pyproj.Proj(proj=\"merc\", lat_ts=data.latitude.mean())\neasting, northing = projection(data.longitude.values, data.latitude.values)\ncoordinates = (easting, northing, data.altitude_m)\n\n# Create the equivalent sources.\n# We'll use block-averaged sources at a constant depth beneath the observation\n# points. We will interpolate on a grid with a resolution of 500m, so we will\n# use blocks of the same size. The damping parameter helps smooth the predicted\n# data and ensure stability.\neqs = hm.EquivalentSources(depth=1000, damping=1, block_size=500, depth_type=\"constant\")\n\n# Fit the sources coefficients to the observed magnetic anomaly.\neqs.fit(coordinates, data.total_field_anomaly_nt)\n\n# Evaluate the data fit by calculating an R\u00b2 score against the observed data.\n# This is a measure of how well the sources fit the data, NOT how good the\n# interpolation will be.\nprint(\"R\u00b2 score:\", eqs.score(coordinates, data.total_field_anomaly_nt))\n\n# Interpolate data on a regular grid with 500 m spacing. The interpolation\n# requires the height of the grid points (upward coordinate). By passing in\n# 1500 m, we're effectively upward-continuing the data (mean flight height is\n# 500 m).\ngrid = eqs.grid(upward=1500, spacing=500, data_names=[\"magnetic_anomaly\"])\n\n# The grid is a xarray.Dataset with values, coordinates, and metadata\nprint(\"\\nGenerated grid:\\n\", grid)\n\n# Plot original magnetic anomaly and the gridded and upward-continued version\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 9), sharey=True)\n\n# Get the maximum absolute value between the original and gridded data so we\n# can use the same color scale for both plots and have 0 centered at the white\n# color.\nmaxabs = vd.maxabs(data.total_field_anomaly_nt, grid.magnetic_anomaly.values)\n\nax1.set_title(\"Observed magnetic anomaly data\")\ntmp = ax1.scatter(\n    easting,\n    northing,\n    c=data.total_field_anomaly_nt,\n    s=20,\n    vmin=-maxabs,\n    vmax=maxabs,\n    cmap=\"seismic\",\n)\nplt.colorbar(tmp, ax=ax1, label=\"nT\", pad=0.05, aspect=40, orientation=\"horizontal\")\nax1.set_xlim(easting.min(), easting.max())\nax1.set_ylim(northing.min(), northing.max())\n\nax2.set_title(\"Gridded and upward-continued\")\ntmp = grid.magnetic_anomaly.plot.pcolormesh(\n    ax=ax2,\n    add_colorbar=False,\n    add_labels=False,\n    vmin=-maxabs,\n    vmax=maxabs,\n    cmap=\"seismic\",\n)\nplt.colorbar(tmp, ax=ax2, label=\"nT\", pad=0.05, aspect=40, orientation=\"horizontal\")\nax2.set_xlim(easting.min(), easting.max())\nax2.set_ylim(northing.min(), northing.max())\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}